# tf_bayesian
Transformers Can Do Bayesian Inference


## Overview
Presentation materials

## Discussion Topic 1

IF the findings in the paper generalize broadly, what impact could this have on future Machine Learning approaches?

## Discussion Topic 2

The approach described fits models 100-5,000 times faster than existing methods. What are some other possible advantages to the approach? What are some drawbacks? Which is faster to score (or inference)--an XGBoost model or a large(ish) Transformer model?

## Discussion Topic 3



## Critical Analysis

In their comparison to machine learning methods (XGBoost and CatBoost), the authors note that they only analyzed datasets from MLBenchmark that were not missing data and that had fewer than 100 predictors. They did not indicate whether their approach was limited to only datasets that met these criteria, or whether this was for expediency. If this is a limitation of the approach, this should have been highlighted. 

The paper has not yet been presented at ICLR, and so there have been limited opportunities for critiques. 

## Resource links

Original Article: https://openreview.net/pdf?id=KSugKcbNf9

## Code demonstration

The code has not yet been made availalbe.

## Video Recording

Link to video recording.
